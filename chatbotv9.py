# -*- coding: utf-8 -*-
"""chatbotv9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ELQZdvkmjUNsW0B5XgtwHKrURVtHraWa
"""

import re
import random

#Pending in v8
def thanking(user_response):
    thanking_input = [r'(^th*anks*$)',r'(^th*ank*you*$)']
    thanking_response = ["You are welcome.." , "It's my pleasure to help you!" , "It was great talking to you..."]

    for thank in thanking_input:
        if re.match(thank , user_response) is not None:
            return random.choice(thanking_response) + "Take care!"
def greeting_bye(user_response):
    bye_inputs = [r'b*y*e*' , r'b*u*y*e*']
    bye_response = ["Bye, " , "It was great talking to you..." , "Bye bye, "]
    
    for greet in bye_inputs:
        if re.match(greet , user_response) is not None:
            return random.choice(bye_response) + "Take care!"
def initiate_chat():
    greet = ["Hello, " , "Hi, " , "Hey, "]
    intro = ["here to assist you with anything related to the Department. " , "ask me anything you want to know about the Department. " , "here to answer your queries about the Department. "]
    ending = ["Let me know how I can help you!" , "Tell me what I can let you know!" , "Let me know what you are interested in!"]
    
    print(random.choice(greet) + "welcome to ISE Department of DSCE. I'm ISEBOT, " + random.choice(intro) + random.choice(ending))

def greeting(user_response):
    greeting_inputs = [r'(^h*i*$)' , r'(^h*el*o*$)' , r'(^h*e*y*$)']
    greeting_reply = ["Hello" , "Hi" , "Anneyeonghaseyo", "Heyy"]
    
    for greet in greeting_inputs:
        if re.match(greet , user_response) is not None:
            return random.choice(greeting_reply) + ", how may I help you?"

import io
import random
import string # to process standard python strings
import warnings
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')
import requests
from bs4 import BeautifulSoup
import re

import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('popular', quiet=True) # for downloading packages

lemmer = nltk.stem.WordNetLemmatizer()

r = requests.get("https://www.dsce.edu.in/academics/ug/ise")

soup = BeautifulSoup(r.content, 'html5lib')

file = open("dsce.txt", "w")
#16400:
data = soup.get_text()[16400:26620].strip()

file.write(data.lower()) 
file.close()

file = open("dsce.txt", "a")
file.write("endddddddddddddddddddddddddd")
file.close()

def getBotResponse(identified_query , navbars , headings3):
    if identified_query[0] == 0:
        solution = headings3[identified_query[1]][0]
        r = requests.get("https://www.dsce.edu.in/academics/ug/ise")
        soup = BeautifulSoup(r.content, 'html5lib')
        table = soup.find('div', attrs = {'id':'content_main'})
        
        for row in table.findAll('div'):
            try:
                if row.h3.text.lower() == solution:
                    print(row.p.text)
                    if row.p.text is None:
                        print("None")
            except:
                continue        
    else:
        solution = navbars[identified_query[1]][0]
        r = requests.get("https://www.dsce.edu.in/academics/ug/ise")
        soup = BeautifulSoup(r.content, 'html5lib')
        table = soup.find('div', attrs = {'id':'content_main'})  
        
        if navbars[identified_query[1]][1].startswith("#"):
            try:
                r = requests.get("https://www.dsce.edu.in/academics/ug/ise")
                soup = BeautifulSoup(r.content, 'html5lib')
                table = soup.find('div')

                try:
                    for row1 in table.findAll('div' , attrs = {'class' : 'col-xs-12 col-md-7 menuid'}):
                        print(row1.text)
                except:
                    print("herree")


            except:
                print("here")
          

        elif navbars[identified_query[1]][1].startswith("/"):
            status = 0
            try:
                #alumni
                new_url = "https://www.dsce.edu.in/" + str(navbars[identified_query[1]][1])
                r = requests.get(new_url)
                soup = BeautifulSoup(r.content, 'html5lib')
                table = soup.find('div', attrs = {'id':'content_main'})
                    
                for row in table.findAll('div' , attrs = {'class' : 'col-sm-12 col-md-8 col-xs-12'}):
                    print(row.h4.text)
                    status = 1
                    for row1 in row.findAll('p'):
                        print(row1.text)
                    print("\n\n")
            except:
                pass
            try:
                    try:
                        new_url = "https://www.dsce.edu.in/" + str(navbars[identified_query[1]][1])
                        r = requests.get(new_url)
                        soup = BeautifulSoup(r.content, 'html5lib')
                        table = soup.find('div', attrs = {'id':'content_main'})
                        for row in table.findAll('table'):
                            for tdrow in row.findAll('td'):
                                print(tdrow.text)
                                status = 1
                    except:
                        pass
                    if status == 0:
                        try:
                            new_url = "https://www.dsce.edu.in/" + str(navbars[identified_query[1]][1])
                            r = requests.get(new_url)
                            soup = BeautifulSoup(r.content, 'html5lib')
                            table = soup.find('div', attrs = {'id':'content_main'})
                            for row in table.findAll('div' , attrs = {'class' : 'itemContainer'}):
                                print(row.h3.a.text.strip() , end = ' - ')
                                print(row.ul.li.text.title())
                                status = 1
                        except:
                            pass
            except:
                print("here")

def dataCollection():
    r = requests.get("https://www.dsce.edu.in/academics/ug/ise")

    soup = BeautifulSoup(r.content, 'html5lib')

    table = soup.find('div', attrs = {'id':'content_main'})

    headings3 = []
    for row in table.findAll('h3'):
        if row.text.lower() not in ["" , " " , "view more" , "ise department"]:
            headings3.append([row.text.lower() , 'h3'])

    for row in table.findAll('ul', attrs = {'class':'nav navbar-nav'}):
        navbars = []
        rejected = []
        hash_dest = []
        for row1 in row.findAll('li'):
            topic = row1.a.text.lower()

            if topic == 'home' or topic in headings3:
                continue

            destination = row1.a['href'].lower()

            try:
                if row1.a['target'] == '_blank':
                    rejected.append([topic , destination])
            except:
                if destination == '#' and topic not in headings3:
                    hash_dest.append([topic , destination])
                else:
                    navbars.append([topic , destination])
    return navbars , headings3

navbars , headings3 = dataCollection()

file = open("dsce.txt", "w")
r = requests.get("https://www.dsce.edu.in/academics/ug/ise")
soup = BeautifulSoup(r.content, 'html5lib')
table = soup.find('div', attrs = {'id':'content_main'})

for row in table.findAll('div'):
            file.write(row.get_text())
file.close()

file = open("dsce.txt", "w")
r = requests.get("https://www.dsce.edu.in/academics/ug/ise")
soup = BeautifulSoup(r.content, 'html5lib')
table = soup.find('div', attrs = {'id':'content_main'})
for row in table.findAll('a'):
    if row['href'] != '/academics/ug/ise' and not row['href'].endswith('.pdf') and not row['href'].endswith('.jpg') and not row['href'].startswith('javascript:void'):
        file.write(row.text)
        file.write("\n")
        new_url = "https://www.dsce.edu.in" + row['href']
        r = requests.get(new_url)
        soup = BeautifulSoup(r.content, 'html5lib')
        try:
            data = row.p.text
            file.write(data)
            file.write("\n\n\n")
        except:
            print(new_url)
            #print(data)
file.close()

folder=open('dsce.txt','r',errors = 'ignore')
raw=folder.read()
raw = raw.lower()# converts to upper case
print(raw)

senttokens = nltk.sent_tokenize(raw)# converts to list of sentences
wordtokens = nltk.word_tokenize(raw)# converts to list of words

lemmer = nltk.stem.WordNetLemmatizer()
#WordNet is a semantically-oriented dictionary of English included in NLTK.
def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]

dict_remove_punct = dict((ord(punct), None) for punct in string.punctuation)

def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(dict_remove_punct)))

def response(user_response):
    ATROBOT_response=''
    senttokens.append(user_response)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(senttokens)
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx=vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    if(req_tfidf==0):
        ATROBOT_response=ATROBOT_response+"I am sorry! I don't understand you"
        return ATROBOT_response
    else:
        ATROBOT_response =  ATROBOT_response+senttokens[idx]
        return ATROBOT_response

def get_response(user_response):
    if greeting(user_response) is None and thanking(user_response) is None and greeting_bye(user_response) is None:
        pass
    elif greeting(user_response) is not None:
        return greeting(user_response)
    elif thanking(user_response) is not None:
        return thanking(user_response)
    else:
        response(user_response)
